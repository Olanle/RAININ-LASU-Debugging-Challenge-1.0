{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9vMhGuPOPF0a",
        "VDKw_mC6RIV4",
        "6MXrDR9KTuUR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 â€“ Python Code Debugging Activity"
      ],
      "metadata": {
        "id": "9vMhGuPOPF0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§  Challenge Description\n",
        "\n",
        "You are given a Python script that processes a list of student scores and calculates their average, highest, and lowest score.\n",
        "However, the code has several bugs that prevent it from running correctly or producing the right results.\n",
        "\n",
        "Your task is to:\n",
        "1. Debug the code so it runs without errors.\n",
        "\n",
        "2. Ensure it prints the correct results.\n",
        "\n",
        "3. Clean up and organize the code for readability.\n",
        "\n",
        "4. Write short comments explaining the main fixes you made."
      ],
      "metadata": {
        "id": "pIEO5UOTPcZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [89, 76, 90, 65, 100, 82]\n",
        "\n",
        "def average(scores):\n",
        "    total = 0\n",
        "    for i in scores:\n",
        "        total += i\n",
        "    avg = total / len(score)\n",
        "    return avg\n",
        "\n",
        "def highest_score(scores):\n",
        "    high = 0\n",
        "    for s in scores:\n",
        "        if s > high:\n",
        "            high = s\n",
        "    return high\n",
        "\n",
        "def lowest_score(scores):\n",
        "    low = 0\n",
        "    for s in score:\n",
        "        if s < low:\n",
        "            low = s\n",
        "    return low\n",
        "\n",
        "print(\"Average Score:\", average(scores))\n",
        "print(\"Highest Score:\", highest_score(scores))\n",
        "print(\"Lowest Score:\", lowest_score(score))\n"
      ],
      "metadata": {
        "id": "ASlDswLbPBDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Correct Output:\n",
        "\n",
        "Average Score: 83.66666666666667\n",
        "\n",
        "Highest Score: 100\n",
        "\n",
        "Lowest Score: 65\n"
      ],
      "metadata": {
        "id": "9l2zCl4kP5nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 â€“ Python Debugging (Intermediate)"
      ],
      "metadata": {
        "id": "VDKw_mC6RIV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§  Challenge Description\n",
        "\n",
        "You are given a Python script that manages studentsâ€™ test scores and calculates the class average, the top-performing student, and the number of students who passed.\n",
        "\n",
        "The code contains logic and syntax errors that prevent it from running or producing the right results.\n",
        "\n",
        "Your task is to:\n",
        "1. Fix all syntax and logic errors.\n",
        "\n",
        "2. Ensure it produces the correct results.\n",
        "\n",
        "3. Clean and organize the code for readability.\n",
        "\n",
        "4. Add short comments explaining your main fixes."
      ],
      "metadata": {
        "id": "hLEtVNPyRIV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = {\n",
        "    'John': 78,\n",
        "    'Mary': 85,\n",
        "    'Emma': 59,\n",
        "    'James': 92,\n",
        "    'Sophia': 64\n",
        "}\n",
        "\n",
        "def class_average(scores):\n",
        "    total = 0\n",
        "    for score in students:\n",
        "        total += students\n",
        "    return total / len(students)\n",
        "\n",
        "def top_student(students):\n",
        "    highest = 0\n",
        "    name = ''\n",
        "    for s, score in students.items:\n",
        "        if score > highest:\n",
        "            name = s\n",
        "            highest = score\n",
        "    return name, score\n",
        "\n",
        "def passed_count(students):\n",
        "    passed = 0\n",
        "    for s in students.keys():\n",
        "        if students[s] > 60:\n",
        "            passed +z 1\n",
        "    return passed\n",
        "\n",
        "print(\"Class Average:\", class_average(students))\n",
        "print(\"Top Student:\", top_student(students))\n",
        "print(\"Number of Students Passed:\", passed_count)"
      ],
      "metadata": {
        "id": "jUnIW-6_XlJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8937a0-f0e0-4bf3-b74f-447444d72105"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Average: 75.6\n",
            "Top Student: ('James', 64)\n",
            "Number of Students Passed: <function passed_count at 0x7f5eec0bf920>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Correct Output:\n",
        "\n",
        "Class Average: 75.6\n",
        "\n",
        "Top Student: ('James', 92)\n",
        "\n",
        "Number of Students Passed: 4"
      ],
      "metadata": {
        "id": "t-Vdxk54RIV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3 â€“ Machine Learning Debugging Activity"
      ],
      "metadata": {
        "id": "6MXrDR9KTuUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ§  Challenge Description\n",
        "\n",
        "You are given a Python script that trains a simple logistic regression model to predict whether a student passes or fails based on their study hours.\n",
        "\n",
        "However, the code contains multiple bugs in data handling, model fitting, and prediction steps.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "1. Debug and fix the code so it runs successfully.\n",
        "\n",
        "2. Ensure the model trains and predicts correctly.\n",
        "\n",
        "3. Clean and organize the code for readability.\n",
        "\n",
        "4. Add short comments explaining your major fixes."
      ],
      "metadata": {
        "id": "XX8Q1wLfTuUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Creating simple dataset\n",
        "data = {\n",
        "    'study_hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'passed': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and labels\n",
        "X = df['study_hours']\n",
        "y = df['passed']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model training\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n",
        "predictions = [1 if x > 0.5 else 0 for x in prediction]\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(\"Model Accuracy:\", acc)\n"
      ],
      "metadata": {
        "id": "dIq24gGdTuUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "05c63491-2d90-455b-902b-37e6764cc2ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1143080528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m   1223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1091\u001b[0m                         \u001b[0;34m\"if it contains a single sample.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                     )\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"USV\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Correct Output:\n",
        "\n",
        "Model Accuracy: 1.0\n"
      ],
      "metadata": {
        "id": "sE2B7-0PTuUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 Correct Code"
      ],
      "metadata": {
        "id": "b0Zt3NoKWrA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [89, 76, 90, 65, 100, 82]\n",
        "\n",
        "def average(scores):\n",
        "    total = 0\n",
        "    for i in scores:\n",
        "        total += i\n",
        "    avg = total / len(scores)\n",
        "    return avg\n",
        "\n",
        "def highest_score(scores):\n",
        "    high = 0\n",
        "    for s in scores:\n",
        "        if s > high:\n",
        "            high = s\n",
        "    return high\n",
        "\n",
        "def lowest_score(scores):\n",
        "    low = 0\n",
        "    for s in scores:\n",
        "        if s < low:\n",
        "            low = s\n",
        "    return low\n",
        "\n",
        "print(\"Average Score:\", average(scores))\n",
        "print(\"Highest Score:\", highest_score(scores))\n",
        "print(\"Lowest Score:\", lowest_score(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SvamErmWtuQ",
        "outputId": "e13b4ca9-a470-43f8-bf98-04e1a8a6d4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Score: 83.66666666666667\n",
            "Highest Score: 100\n",
            "Lowest Score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 Correct Code"
      ],
      "metadata": {
        "id": "RgRvHDQJXCbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = {\n",
        "    'John': 78,\n",
        "    'Mary': 85,\n",
        "    'Emma': 59,\n",
        "    'James': 92,\n",
        "    'Sophia': 64\n",
        "}\n",
        "\n",
        "def class_average(students):\n",
        "    total = 0\n",
        "    for score in students.values():  # Loop through dictionary values\n",
        "        total += score\n",
        "    return total / len(students)\n",
        "\n",
        "def top_student(students):\n",
        "    highest = 0\n",
        "    name = ''\n",
        "    for s, score in students.items():  # Use .items() to access both key and value\n",
        "        if score > highest:\n",
        "            name = s\n",
        "            highest = score\n",
        "    return name, highest\n",
        "\n",
        "def passed_count(students):\n",
        "    passed = 0\n",
        "    for score in students.values():\n",
        "        if score > 60:\n",
        "            passed += 1  # Increment counter properly\n",
        "    return passed\n",
        "\n",
        "# Printing results\n",
        "print(\"Class Average:\", class_average(students))\n",
        "print(\"Top Student:\", top_student(students))\n",
        "print(\"Number of Students Passed:\", passed_count(students))"
      ],
      "metadata": {
        "outputId": "acb6ef12-e052-4a7f-e8fc-6e76fc97b570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMR4IZc4XCbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Average: 75.6\n",
            "Top Student: ('James', 92)\n",
            "Number of Students Passed: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3 Correct Code"
      ],
      "metadata": {
        "id": "iYfo2hnvYGtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression  # âœ… Correct model for classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a simple dataset\n",
        "data = {\n",
        "    'study_hours': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'passed': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Features and labels (X must be 2D)\n",
        "X = df[['study_hours']]  # Double brackets make it a DataFrame\n",
        "y = df['passed']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "print(\"Model Accuracy:\", acc)\n"
      ],
      "metadata": {
        "outputId": "c00981ff-ea5d-49f0-aa29-cdef06a27f46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMf2HGptYGtT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}